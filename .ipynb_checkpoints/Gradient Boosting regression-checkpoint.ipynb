{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'cadata.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-372829ab21ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                'AveBedrms','Population','AveOccup','Latitude','Longitud']\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cadata.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumnNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Now we have to split the datasets into training and validation. The training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow_py3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'cadata.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# =============\n",
    "# Introduction\n",
    "# =============\n",
    "# I've been doing some data mining lately and specially looking into `Gradient\n",
    "# Boosting Trees <http://en.wikipedia.org/wiki/Gradient_boosting>`_ since it is\n",
    "# claimed that this is one of the techniques with best performance out of the\n",
    "# box.  In order to have a better understanding of the technique I've reproduced\n",
    "# the example of section *10.14.1 California Housing* in the book `The Elements of Statistical Learning <http://www-stat.stanford.edu/~tibs/ElemStatLearn/>`_.\n",
    "# Each point of this dataset represents the house value of a property with some\n",
    "# attributes of that house. You can get the data and the description of those\n",
    "# attributes from `here <http://lib.stat.cmu.edu/modules.php?op=modload&name=Downloads&file=index&req=getit&lid=83>`_.\n",
    "\n",
    "# I know that the whole exercise here can be easily done with the **R** package\n",
    "# `gbm <http://cran.r-project.org/web/packages/gbm/index.html>`_ but I wanted to\n",
    "# do the exercise using Python. Since learning several languages well enough is\n",
    "# difficult and time consuming I would prefer to stick all my data analysis to\n",
    "# Python instead doing it in R, even with R being superior on some cases. But\n",
    "# having only one language for doing all your scripting, systems programming and\n",
    "# prototyping *PLUS* your data analysis is a good reason for me. Your upfront\n",
    "# effort of learning the language, setting up your tools and editors, etc. is\n",
    "# done only once instead of twice. \n",
    "# \n",
    "# Data Preparation\n",
    "# -----------------\n",
    "# The first thing to do is to load the data into a `Pandas <http://pandas.pydata.org/pandas-docs/stable/>`_  dataframe\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "columnNames = ['HouseVal','MedInc','HouseAge','AveRooms',\n",
    "               'AveBedrms','Population','AveOccup','Latitude','Longitud']\n",
    "\n",
    "df = pd.read_csv('/home/johan/repos/GitHub/Introduction-to-Machine-Learning/Datasets/cadata.txt',skiprows=27, sep='\\s+',names=columnNames)\n",
    "\n",
    "# Now we have to split the datasets into training and validation. The training\n",
    "# data will be used to generate the trees that will constitute the final\n",
    "# averaged model.\n",
    "\n",
    "import random\n",
    "\n",
    "X = df[df.columns - ['HouseVal']]\n",
    "Y = df['HouseVal']\n",
    "rows = random.sample(df.index, int(len(df)*.80))\n",
    "x_train, y_train = X.ix[rows],Y.ix[rows]\n",
    "x_test,y_test  = X.drop(rows),Y.drop(rows)\n",
    "\n",
    "# We then fit a Gradient Tree Boosting model to the data using the\n",
    "# `scikit-learn <http://scikit-learn.org/stable/>`_ package. We will use 500 trees\n",
    "# with each tree having a depth of 6 levels. In order to get results similar to\n",
    "# those in the book we also use the `Huber loss function <http://en.wikipedia.org/wiki/Huber_loss_function>`_ .\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "params = {'n_estimators': 500, 'max_depth': 6,\n",
    "        'learning_rate': 0.1, 'loss': 'huber','alpha':0.95}\n",
    "clf = GradientBoostingRegressor(**params).fit(x_train, y_train)\n",
    "\n",
    "# For me, the Mean Squared Error wasn't much informative and used instead the\n",
    "# :math:`R^2` **coefficient of determination**. This measure is a number\n",
    "# indicating how well a variable is able to predict the other. Numbers close to\n",
    "# 0 means poor prediction and numbers close to 1 means perfect prediction. In the\n",
    "# book, they claim a 0.84 against a 0.86 reported in the paper that created the\n",
    "# dataset using a highly tuned algorithm. I'm getting a good 0.83 without much\n",
    "# tunning of the parameters so it's a good out of the box technique.\n",
    "\n",
    "mse = mean_squared_error(y_test, clf.predict(x_test))\n",
    "r2 = r2_score(y_test, clf.predict(x_test))\n",
    "\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"R2: %.4f\" % r2)\n",
    "\n",
    "# Let's plot how does it behave the training and testing error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_decision_function(x_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "                label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "                label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "# As you can see in the previous graph, although the train error keeps going\n",
    "# down as we add more trees to our model, the test error remains more or less\n",
    "# constant and doesn't incur in overfitting. This is mainly due to the shrinkage\n",
    "# parameter and one of the good features of this algorithm.\n",
    "\n",
    "\n",
    "# When doing data mining as important as finding a good model is being able to\n",
    "# interpret it, because based on that analysis and interpretation preemptive\n",
    "# actions can be performed. Although base trees are easily interpretable when\n",
    "# you are adding several of those trees interpretation is more difficult. You\n",
    "# usually rely on some measures of the predictive power of each feature. Let's\n",
    "# plot feature importance in predicting the House Value.\n",
    "\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n",
    "\n",
    "# Once variable importance has been identified we could try to investigate how\n",
    "# those variables interact between them. For instance, we can plot the\n",
    "# dependence of the target variable with another variable has been averaged over\n",
    "# the values of the other variables not being taken into consideration. Some\n",
    "# variables present a clear monotonic dependence with the target value, while\n",
    "# others seem not very related to the target variable even when they ranked high\n",
    "# in the previous plot. This could be signaling an interaction between variables\n",
    "# that could be further studied. \n",
    "\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "fig, axs = plot_partial_dependence(clf, x_train, \n",
    "                                   features=[3,2,7,6],\n",
    "                                   feature_names=x_train.columns,\n",
    "                                   n_cols=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# The last step performed was to explore the capabilities of the Python\n",
    "# libraries when plotting data in a map. Here we are plotting the predicted\n",
    "# House Value in California using Latitude and Longitude as the axis for\n",
    "# plotting this data in the map.\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "predDf = pd.DataFrame(x_test.copy())\n",
    "predDf['y_pred'] = clf.predict(x_test)\n",
    "\n",
    "def california_map(ax=None, lllat=31.5,urlat=42.5,\n",
    "                   lllon=-124,urlon=-113):\n",
    "    m = Basemap(ax=ax, projection='stere',\n",
    "                lon_0=(urlon + lllon) / 2,\n",
    "                lat_0=(urlat + lllat) / 2,\n",
    "                llcrnrlat=lllat, urcrnrlat=urlat,\n",
    "                llcrnrlon=lllon, urcrnrlon=urlon,\n",
    "                resolution='f')\n",
    "    m.drawstates()\n",
    "    m.drawcountries()\n",
    "    m.drawcoastlines(color='lightblue')\n",
    "    return m\n",
    "\n",
    "plt.figure()\n",
    "m= california_map()\n",
    "predDf = predDf.sort('y_pred') # Useful for plotting\n",
    "x,y = m(predDf['Longitud'],predDf['Latitude'])\n",
    "serieA = (np.array(predDf['y_pred']) - predDf['y_pred'].min())/(predDf['y_pred'].max()-predDf['y_pred'].min())\n",
    "# z = plt.cm.jet(serieA)\n",
    "z = np.array(predDf['y_pred'])/1000\n",
    "m.scatter(x,y,c=z,s=60,alpha=0.5,edgecolors='none')\n",
    "c = m.colorbar(location='right')\n",
    "c.set_label(\"House Value (Thousands of $)\")\n",
    "m.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Addendum\n",
    "# --------\n",
    "# This blog post was written using `Pylit <http://pylit.berlios.de/>`_ as the tool\n",
    "# for doing `Literate Programming <http://en.wikipedia.org/wiki/Literate_programming>`_. \n",
    "# I think that literate programming is way superior to other software\n",
    "# methodologies like TDD when coding algorithms for data analysis. The main\n",
    "# problem I find right now is the lack of tooling for really taking advantage of\n",
    "# literate programming, but this is a technique that I'm definitely going to\n",
    "# research deepe I think that literate programming is way superior to other\n",
    "# software methodologies like TDD when coding algorithms for data analysis. The\n",
    "# main problem I find right now is the lack of tooling for really taking\n",
    "# advantage of literate programming, but this is a technique that I'm definitely\n",
    "# going to research deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
