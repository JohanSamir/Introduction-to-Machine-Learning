{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Ridge Regression, the LASSO, and ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of controlling the capacity of Neural Networks to prevent overfitting:\n",
    "\n",
    "**L2 regularization** is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. As we discussed in the Linear Classification section, due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. Lastly, notice that during gradient descent parameter update, using the L2 regularization ultimately means that every weight is decayed linearly: W += -lambda * W towards zero.\n",
    "\n",
    "**L1 regularization** is another relatively common form of regularization, where for each weight w\n",
    "we add the term λ∣w∣ to the objective. It is possible to combine the L1 regularization with the L2 regularization: λ1∣w∣+λ2w^2, **(this is called Elastic net regularization)**. The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.\n",
    "\n",
    "<img src=\"images/q.png\" style=\"width: 450px;\">\n",
    "<img src=\"images/norm_balls.png\" style=\"width: 450px;\">\n",
    "\n",
    "<img src=\"images/qqa.png\" style=\"width: 450px;\">\n",
    "<img src=\"images/elastic.png\" style=\"width: 450px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization techniques encourage models to have a preference towards simpler models without making a part of the hypothesis space completely unavailable. These techniques intend to reduce the risk of overfitting without increasing the bias significantly.\n",
    "\n",
    "There are several ways to regularize a model. A very common regularization strategy is to add a weight decay term to the loss function. This weight decay term puts a constraint on the model complexity by penalizing large weights. For example, let's say our original loss function is the mean squared error and we add the sum of squares of the weights to this loss function as a weight decay term. Here, alpha is a hyperparameter that determines the strength of the regularization. Setting alpha to zero basically disables the regularizer. On the other hand, a large alpha may lead to underfitting.\n",
    "\n",
    "A key property of L1-regularization is that it leads to sparser weights. In other words, it drives less important weights to zero, therefore acting like a natural feature selector.\n",
    "\n",
    "The reason behind that is that L2 regularization penalizes smaller weights less than the larger weights since it tries to minimize the squared magnitude of the weights. So, there isn't really a big incentive for the model to drive the small weights to zero. For example, reducing a weight from a to b decreases the loss greatly where reducing a smaller weight by the same amount decreases the loss by a much smaller amount.\n",
    "\n",
    "L1 regularization, on the other hand, pushes all the weights down equally. As a result, some weights get smashed down to zero and only a subset of the weights survives. Although L1-regularization has this nice sparsity property, personally, I have rarely seen it lead to a significantly better performance than L2-regularization. So, I would say L2-regularization is pretty much the go-to option unless you have a reason to use L1.\n",
    "\n",
    "Early stopping has a regularization effect that is similar to weight decay. Given that we initialize our model with small weights, every training iteration has the potential to update a weight towards larger values. Therefore, the earlier we stop the training, the smaller the weights are likely to be.\n",
    "\n",
    "Regularization methods, in general, introduce additional prior knowledge in optimization processes. For example, weight decay and early stopping encode the additional information that smaller weights are preferable to larger ones. \n",
    "\n",
    "<img src=\"images/qq.png\" style=\"width: 450px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression with l1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\";\")\n",
    "    #scale real values\n",
    "    for col in df.columns[:-1]:\n",
    "        if df[col].dtype == int:\n",
    "            df[col] = StandardScaler().fit_transform(np.array(df[col],dtype=np.float).reshape(-1,1)).reshape(1, -1).tolist()[0]\n",
    "    #one hot encode categorical features\n",
    "    columns = df.columns.tolist()\n",
    "    obj_cols = [col for col in columns[:-1] if df[col].dtype==object]\n",
    "    for col in obj_cols:\n",
    "        df = pd.concat([pd.get_dummies(df[col], prefix=col), df], axis=1)#add dummies of a column\n",
    "        df.drop(col, axis=1, inplace=True)#remove original column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"/home/johan/repos/GitHub/Introduction-to-Machine-Learning/Datasets/bank.csv\")\n",
    "features = df[df.columns[:-1]]\n",
    "labels = df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.score: 0.8963531669865643\n"
     ]
    }
   ],
   "source": [
    "l1 = LogisticRegression(penalty='l1')\n",
    "l1.fit(features[:4000], labels[:4000])\n",
    "print('l1.score:',l1.score(features[4000:], labels[4000:]))\n",
    "w1 = np.array(l1.coef_, copy=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression with l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.score: 0.8963531669865643\n"
     ]
    }
   ],
   "source": [
    "l2 = LogisticRegression(penalty='l2')\n",
    "l2.fit(features[:4000], labels[:4000])\n",
    "print('l1.score:',l2.score(features[4000:], labels[4000:]))\n",
    "w2 = np.array(l2.coef_, copy=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print (sum(abs(w2 - 0.0) < 0.000000000001))\n",
    "print (sum(abs(w1 - 0.0) < 0.000000000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features.\n",
    "\n",
    "Comparing equations (8.6) and (8.3) we can see that both L1 and L2 Regularizations lead to a reduction in the weights with each iteration. However the way the weights drop is different: In L2 Regularization the weight reduction is multiplicative and proportional to the value of the weight, so it is faster for large weights and de-accelerates as the weights get smaller. In L1 Regularization on the other hand, the weights are reduced by a fixed amount in every iteration, irrespective of the value of the weight. Hence for larger weights L2 Regularization is faster than L1, while for smaller weights the reverse is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://cs231n.github.io/neural-networks-2/\n",
    "2. https://icml.cc/Conferences/2004/proceedings/papers/354.pdf\n",
    "3. https://www.slideshare.net/kaz_yos/visual-explanation-of-ridge-regression-and-lasso\n",
    "4. http://www.ds100.org/sp17/assets/notebooks/linear_regression/Regularization.html\n",
    "5. https://srdas.github.io/DLBook/ImprovingModelGeneralization.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
